{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(R\"..\\data-fetcher\\data\\total_data.csv\")\n",
    "df_with_hours = df.loc[df['target'].notnull()][['time','hit']]\n",
    "hours = [1,2,4,8]\n",
    "for hour in hours:\n",
    "    df_with_hours[str(hour) + '_hour'] = df_with_hours.apply(lambda x: 1 if((x['hit'] - x['time']) < hour*3600) else 0 , axis = 'columns')\n",
    "df_with_hours.drop(['time','hit'], axis='columns',inplace=True)\n",
    "df = df.join(df_with_hours)\n",
    "df[df[['1_hour','2_hour','4_hour','8_hour']].notnull().any(axis= 'columns')]\n",
    "df.to_csv(R\"..\\data-fetcher\\data\\total_data_with_hours.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(R\"..\\data-fetcher\\data\\total_data_with_hours.csv\",index_col = 0)\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'], unit = \"s\")\n",
    "df.set_index(pd.DatetimeIndex(df['time']), inplace = True)\n",
    "df = df.drop('time', axis= 'columns')\n",
    "\n",
    "def filter_weekend_hours(df):\n",
    "    # Extract day of the week and hour\n",
    "    df['day_of_week'] = df.index.dayofweek  # Monday=0, Sunday=6\n",
    "    df['hour'] = df.index.hour\n",
    "\n",
    "    # Define the condition to remove entries between Friday 8 PM to Monday 4 AM\n",
    "    condition = ~(\n",
    "        ((df['day_of_week'] == 4) & (df['hour'] >= 20)) |  # Last 4 hours of Friday\n",
    "        (df['day_of_week'] == 5) |                         # Entire Saturday\n",
    "        (df['day_of_week'] == 6) |                         # Entire Sunday\n",
    "        ((df['day_of_week'] == 0) & (df['hour'] < 4))      # First 4 hours of Monday\n",
    "    )\n",
    "\n",
    "    # Apply filter and drop temporary columns\n",
    "    filtered_df = df[condition].drop(columns=['day_of_week', 'hour'])\n",
    "    return filtered_df\n",
    "\n",
    "# Apply the function\n",
    "filtered_df = filter_weekend_hours(df)\n",
    "filtered_df.loc[filtered_df.index.dayofweek == 0]\n",
    "filtered_df.to_csv(R\"..\\data-fetcher\\data\\total_data_with_hours.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Earliest Times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_99864\\2257347282.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  earliest_times = df.groupby('day_of_week').apply(lambda x: x.index.min())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "day_of_week\n",
       "Monday      2024-11-25 14:56:00\n",
       "Tuesday     2024-11-26 00:00:00\n",
       "Wednesday   2024-11-27 00:00:00\n",
       "Thursday    2024-11-28 00:00:00\n",
       "Friday      2024-11-29 00:00:00\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_earliest_times_by_day(df):\n",
    "    \"\"\"\n",
    "    Finds the earliest time for each day of the week in a DataFrame with a datetime index.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame with a datetime index.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: A series with the days of the week as the index and their earliest times as values.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame has a datetime index\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"The DataFrame must have a datetime index.\")\n",
    "    \n",
    "    # Extract the day of the week\n",
    "    df['day_of_week'] = df.index.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "    # Group by day_of_week and find the earliest time for each day\n",
    "    earliest_times = df.groupby('day_of_week').apply(lambda x: x.index.min())\n",
    "\n",
    "    # Map day_of_week to day names for readability\n",
    "    day_names = {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\", \n",
    "                 4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"}\n",
    "    earliest_times.index = earliest_times.index.map(day_names)\n",
    "\n",
    "    return earliest_times\n",
    "get_earliest_times_by_day(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DatetimeIndex(...) must be called with a collection of some kind, None was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m filtered_df\u001b[38;5;241m.\u001b[39mset_index(pd\u001b[38;5;241m.\u001b[39mDatetimeIndex(filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]), inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m filtered_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m filtered_df\u001b[38;5;241m.\u001b[39mloc[pd\u001b[38;5;241m.\u001b[39mDatetimeIndex()]\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:352\u001b[0m, in \u001b[0;36mDatetimeIndex.__new__\u001b[1;34m(cls, data, freq, tz, normalize, closed, ambiguous, dayfirst, yearfirst, dtype, copy, name)\u001b[0m\n\u001b[0;32m    344\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalize\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m construction is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will be removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    348\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    349\u001b[0m     )\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(data):\n\u001b[1;32m--> 352\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_scalar_data_error(data)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# - Cases checked above all return/raise before reaching here - #\u001b[39;00m\n\u001b[0;32m    356\u001b[0m name \u001b[38;5;241m=\u001b[39m maybe_extract_name(name, data, \u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5289\u001b[0m, in \u001b[0;36mIndex._raise_scalar_data_error\u001b[1;34m(cls, data)\u001b[0m\n\u001b[0;32m   5284\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   5285\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   5286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_scalar_data_error\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data):\n\u001b[0;32m   5287\u001b[0m     \u001b[38;5;66;03m# We return the TypeError so that we can raise it from the constructor\u001b[39;00m\n\u001b[0;32m   5288\u001b[0m     \u001b[38;5;66;03m#  in order to keep mypy happy\u001b[39;00m\n\u001b[1;32m-> 5289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   5290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(...) must be called with a collection of some \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5291\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(data)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28misinstance\u001b[39m(data,\u001b[38;5;250m \u001b[39mnp\u001b[38;5;241m.\u001b[39mgeneric)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mstr\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas passed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5293\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: DatetimeIndex(...) must be called with a collection of some kind, None was passed"
     ]
    }
   ],
   "source": [
    "filtered_df = pd.read_csv(R\"..\\data-fetcher\\data\\total_data_with_hours.csv\")\n",
    "\n",
    "filtered_df.set_index(pd.DatetimeIndex(filtered_df['time']), inplace = True)\n",
    "filtered_df = filtered_df.drop('time', axis= 'columns')\n",
    "filtered_df = filtered_df.loc[pd.DatetimeIndex()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m prev_bars \u001b[38;5;241m=\u001b[39m filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[i \u001b[38;5;241m-\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(minutes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m):i \u001b[38;5;241m-\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(minutes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[0;32m     15\u001b[0m prev_ma \u001b[38;5;241m=\u001b[39m filtered_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoving_average\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[i \u001b[38;5;241m-\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(minutes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m60\u001b[39m):i \u001b[38;5;241m-\u001b[39m pd\u001b[38;5;241m.\u001b[39mTimedelta(minutes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m---> 18\u001b[0m y_ \u001b[38;5;241m=\u001b[39m filtered_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1_hour\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2_hour\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4_hour\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8_hour\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mloc[i]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(prev_bars) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m60\u001b[39m): \n\u001b[0;32m     23\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mappend(close\u001b[38;5;241m+\u001b[39mprev_bars\u001b[38;5;241m+\u001b[39mprev_ma\u001b[38;5;241m+\u001b[39mbox_data)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\frame.py:4117\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m   4115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 4117\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_with_is_copy(indexer, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[0;32m   4120\u001b[0m     \u001b[38;5;66;03m# What does looking for a single key in a non-unique index return?\u001b[39;00m\n\u001b[0;32m   4121\u001b[0m     \u001b[38;5;66;03m# The behavior is inconsistent. It returns a Series, except when\u001b[39;00m\n\u001b[0;32m   4122\u001b[0m     \u001b[38;5;66;03m# - the key itself is repeated (test on data.shape, #9519), or\u001b[39;00m\n\u001b[0;32m   4123\u001b[0m     \u001b[38;5;66;03m# - we have a MultiIndex on columns (test on self.columns, #21309)\u001b[39;00m\n\u001b[0;32m   4124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n\u001b[0;32m   4125\u001b[0m         \u001b[38;5;66;03m# GH#26490 using data[key] can cause RecursionError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indices\u001b[38;5;241m=\u001b[39mindices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mtake(\n\u001b[0;32m   4134\u001b[0m     indices,\n\u001b[0;32m   4135\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_block_manager_axis(axis),\n\u001b[0;32m   4136\u001b[0m     verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   4137\u001b[0m )\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    895\u001b[0m     new_axis\u001b[38;5;241m=\u001b[39mnew_labels,\n\u001b[0;32m    896\u001b[0m     indexer\u001b[38;5;241m=\u001b[39mindexer,\n\u001b[0;32m    897\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    898\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    899\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    900\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:680\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested axis not found in manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:843\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    841\u001b[0m                     blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    842\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m                 nb \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mtake_nd(taker, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, new_mgr_locs\u001b[38;5;241m=\u001b[39mmgr_locs)\n\u001b[0;32m    844\u001b[0m                 blocks\u001b[38;5;241m.\u001b[39mappend(nb)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m algos\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m   1308\u001b[0m     values, indexer, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m   1309\u001b[0m )\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32mc:\\Users\\siddh\\miniconda3\\envs\\spyder-env\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m func(arr, indexer, out, fill_value)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "box_indices = filtered_df.index[filtered_df['target'].notnull()].to_list()\n",
    "box_indices\n",
    "dataset = []\n",
    "y = []\n",
    "max_length = 0\n",
    "#filtered_df['close'] = scaler.fit_transform(filtered_df['close'])\n",
    "#filtered_df['moving_average'] = scaler.fit_transform(filtered_df['moving_average'])\n",
    "#filtered_df[['high_reward','high_risk','low_reward','low_risk']] = scaler.fit_transform(filtered_df[['high_reward','high_risk','low_reward','low_risk']],axis= 0)\n",
    "for i in box_indices:\n",
    "    close = [filtered_df['close'].loc[i]]\n",
    "    box_data = filtered_df[['high_reward','high_risk','low_reward','low_risk']].loc[i].to_list()\n",
    "\n",
    "\n",
    "    prev_bars = filtered_df['close'].loc[i - pd.Timedelta(minutes = 60):i - pd.Timedelta(minutes = 1)].to_list()\n",
    "    prev_ma = filtered_df['moving_average'].loc[i - pd.Timedelta(minutes = 60):i - pd.Timedelta(minutes = 1)].to_list()\n",
    "    \n",
    "\n",
    "    y_ = filtered_df[['1_hour','2_hour','4_hour','8_hour']].loc[i].to_list()\n",
    "\n",
    "\n",
    "    \n",
    "    if(len(prev_bars) == 60): \n",
    "        dataset.append(close+prev_bars+prev_ma+box_data)\n",
    "        y.append(y_)\n",
    "    else:\n",
    "        continue\n",
    "training_len = int(len(dataset)*0.80)\n",
    "training_data,training_y = scaler.fit_transform(dataset[:training_len]),y[:training_len]\n",
    "testing_data,testing_y = scaler.fit_transform(dataset[training_len:]),y[training_len:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14372"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(box_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "training_set,testing_set = [], []\n",
    "target_train, target_test = [],[]\n",
    "\n",
    "for i in range(training_len - n):\n",
    "    training_set.append(training_data[i:i+n])\n",
    "    target_train.append(training_y[i + n - 1])\n",
    "\n",
    "for i in range(len(testing_data) - n):\n",
    "    testing_set.append(testing_data[i:i+n])\n",
    "    target_test.append(testing_y[i + n - 1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Price_Prediction_Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers,output_size):\n",
    "        super(Price_Prediction_Model, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size=hidden_size, num_layers = num_layers,batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self,x):\n",
    "        x = x.unsqueeze(0)\n",
    "        lstm_out,_ = self.lstm(x)\n",
    "\n",
    "        fc_out = self.fc(lstm_out[:,-1,:])\n",
    "        hit_prob = torch.sigmoid(fc_out)\n",
    "        return hit_prob\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import float32\n",
    "\n",
    "\n",
    "n = 20\n",
    "training_set,testing_set = [], []\n",
    "target_train, target_test = [],[]\n",
    "\n",
    "for i in range(training_len - n):\n",
    "    training_set.append(training_data[i:i+n])\n",
    "    target_train.append(training_y[i + n - 1])\n",
    "\n",
    "for i in range(len(testing_data) - n):\n",
    "    testing_set.append(testing_data[i:i+n])\n",
    "    target_test.append(testing_y[i + n - 1])\n",
    "\n",
    "\n",
    "training_set = torch.tensor(training_set, device = device, dtype=torch.float32)\n",
    "testing_set = torch.tensor(testing_set, device = device, dtype=torch.float32)\n",
    "target_train = torch.tensor(target_train, device = device, dtype=torch.float32)\n",
    "target_test = torch.tensor(target_test, device = device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.06906, validation_loss: 0.01000\n",
      "Epoch: 21, loss: 0.06899, validation_loss: 0.01000\n",
      "Epoch: 41, loss: 0.06892, validation_loss: 0.01000\n",
      "Epoch: 61, loss: 0.06885, validation_loss: 0.01000\n",
      "Epoch: 81, loss: 0.06878, validation_loss: 0.01000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "\n",
    "model = Price_Prediction_Model(125,128,1,4)\n",
    "model = model.to(device)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters())\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred = model(training_set)\n",
    "        \n",
    "\n",
    "    loss = loss_function(pred,target_train)\n",
    "    epoch_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "#    model.eval()\n",
    "#    with torch.no_grad():\n",
    "#        validation_loss = 0\n",
    "#        for sequence,target in zip(testing_set,target_test):\n",
    "#            pred = model(sequence).view(4,)\n",
    "#            loss = loss_function(pred,target)\n",
    "#            validation_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    if((epoch+1)%20 == 1):\n",
    "        print(\"Epoch: %d, loss: %1.5f, validation_loss: %1.5f\" % (epoch+1, epoch_loss/len(training_set), 0.01))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "       layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4770, 0.4781, 0.4812, 0.5308],\n",
      "        [0.4769, 0.4782, 0.4812, 0.5307],\n",
      "        [0.4770, 0.4781, 0.4812, 0.5307],\n",
      "        [0.4769, 0.4781, 0.4812, 0.5307],\n",
      "        [0.4766, 0.4779, 0.4813, 0.5307],\n",
      "        [0.4773, 0.4780, 0.4814, 0.5306],\n",
      "        [0.4772, 0.4782, 0.4813, 0.5306],\n",
      "        [0.4771, 0.4782, 0.4812, 0.5307],\n",
      "        [0.4771, 0.4782, 0.4812, 0.5307],\n",
      "        [0.4771, 0.4782, 0.4811, 0.5307]], device='cuda:0')\n",
      "validation loss: 0.68713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    validation_loss = 0\n",
    "    \n",
    "    pred = model(training_set)\n",
    "    loss = loss_function(pred,target_train)\n",
    "    validation_loss += loss.item()\n",
    "    print(pred)\n",
    "    print(\"validation loss: %1.5f\" % (validation_loss))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
